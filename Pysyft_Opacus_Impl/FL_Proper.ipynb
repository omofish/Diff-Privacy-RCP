{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning on MNIST using a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check if imports were done properly\n",
    "print(torch.__version__)\n",
    "print (torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'c:\\users\\jason\\miniconda3\\envs\\rcp2020\\lib\\site-packages\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.4.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\jason\\miniconda3\\envs\\rcp2020\\lib\\site-packages\\tf_encrypted\\session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice\n",
    "\n",
    "# No secure randomness for this implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 256\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 32\n",
    "        self.save_model = True\n",
    "\n",
    "        # DP\n",
    "        self.using_DP = True\n",
    "\n",
    "        # train_dp\n",
    "        self.stddev = 0.1\n",
    "\n",
    "        # train_dp_2\n",
    "        self.num_microbatches = 64\n",
    "        # Clipping Threshold S\n",
    "        self.S = 1\n",
    "        # noise_multiplier z\n",
    "        self.z = 1.1\n",
    "        self.sigma = self.z * self.S\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and sending to workers\n",
    "We first load the data and transform the training Dataset into a Federated Dataset split across the workers using the `.federate` method. This federated dataset is now given to a Federated DataLoader. The test dataset remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The following options are not supported: num_workers: 1, pin_memory: True\n"
     ]
    }
   ],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN specification\n",
    "Here we use exactly the same CNN as in the official example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the train and test functions\n",
    "\n",
    "For the train function, because the data batches are distributed across `alice` and `bob`, you need to send the model to the right location for each batch. Then, you perform all the operations remotely with the same syntax like you're doing local PyTorch. When you're done, you get back the model updated and the loss to look for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * test_acc))\n",
    "\n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc_no_dp_0': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'test_loss_no_dp_0': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'train_acc_no_dp_0': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'train_loss_no_dp_0': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'test_acc_dp_1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'test_loss_dp_1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'train_acc_dp_1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'train_loss_dp_1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'test_acc_dp_5': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'test_loss_dp_5': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'train_acc_dp_5': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'train_loss_dp_5': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'test_acc_dp_10': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'test_loss_dp_10': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'train_acc_dp_10': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'train_loss_dp_10': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "variants = [0, 1, 5, 10]\n",
    "phases = ['test', 'train']\n",
    "metrics = ['acc', 'loss']\n",
    "history = {}\n",
    "current_exp = 0\n",
    "using_DP = False\n",
    "\n",
    "for v in variants:\n",
    "    d = 'no_dp' if v == 0 else 'dp'\n",
    "    for p in phases:\n",
    "        for m in metrics:\n",
    "            history[f'{p}_{m}_{d}_{str(v)}'] = np.zeros((epochs))\n",
    "\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(using_DP=False, stddev=0):\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
    "    args.using_DP = using_DP\n",
    "    args.stddev = stddev\n",
    "\n",
    "    dp_suffix = 'dp' if args.using_DP else 'no_dp'\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        history[f'train_acc_{dp_suffix}_{str(stddev)}'][epoch - 1], history[f'train_loss_{dp_suffix}_{str(stddev)}'][epoch - 1] = train_dp(args, model, device, federated_train_loader, optimizer, epoch, using_DP)\n",
    "        history[f'test_acc_{dp_suffix}_{str(stddev)}'][epoch - 1], history[f'test_loss_{dp_suffix}_{str(stddev)}'][epoch - 1] = test(args, model, device, test_loader)\n",
    "\n",
    "    if (args.save_model):\n",
    "        torch.save(model.state_dict(), f\"mnist_cnn_fl{dp_suffix}_.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dp_2(args, model, device, federated_train_loader, optimizer, epoch, using_dp):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.CrossEntropyLoss(reduction='none')(output, target)\n",
    "        \n",
    "        # diverge from train here\n",
    "        losses = torch.mean(loss.reshape(4, -1), dim=1)\n",
    "        saved_var = dict()\n",
    "        for tensor_name, tensor in model.named_parameters():\n",
    "            saved_var[tensor_name] = torch.zeros_like(tensor)\n",
    "        \n",
    "        for j in losses:\n",
    "            j.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                print(p)\n",
    "                print(p.device)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.S)\n",
    "            for tensor_name, tensor in model.named_parameters():\n",
    "                new_grad = tensor.grad\n",
    "                saved_var[tensor_name].add_(new_grad)\n",
    "            model.zero_grad()\n",
    "            \n",
    "        for tensor_name, tensor in model.named_parameters():\n",
    "            if device.type =='cuda':\n",
    "                noise = torch.cuda.FloatTensor(tensor.grad.shape).normal_(0, args.sigma)\n",
    "            else:\n",
    "                noise = torch.FloatTensor(tensor.grad.shape).normal_(0, args.sigma)\n",
    "            saved_var[tensor_name].add_(noise)\n",
    "            tensor.grad = saved_var[tensor_name] / num_microbatches\n",
    "        \n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dp(args, model, device, federated_train_loader, optimizer, epoch, using_dp):\n",
    "    print(args.stddev)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "        correct += pred.eq(target.view_as(pred)).sum().get()\n",
    "            \n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # for DP\n",
    "        if(using_dp):\n",
    "            with(torch.no_grad()):\n",
    "                # add gaussian noise to the updates for each layer\n",
    "                for tensor in (model.parameters()):\n",
    "                    # normal dist using given std dev value\n",
    "                    noise = torch.normal(mean=1, std = args.stddev, size=tensor.grad.shape).to(device)\n",
    "                    n_ptr = noise.send(data.location)\n",
    "                    tensor.grad = tensor.grad * n_ptr\n",
    "\n",
    "        optimizer.step()\n",
    "                    \n",
    "        model.get() # <-- NEW: get the model back\n",
    "        \n",
    "        # get metrics\n",
    "        loss = loss.get() # <-- NEW: get the loss back\n",
    "        train_loss += loss.item() * args.batch_size\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "        \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()))\n",
    "        \n",
    "\n",
    "    train_loss /= (len(federated_train_loader) * args.batch_size)\n",
    "    train_acc = correct.item() / (len(federated_train_loader) * args.batch_size)\n",
    "\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        train_loss, correct, len(federated_train_loader) * args.batch_size,\n",
    "        100. * train_acc))\n",
    "\n",
    "    return train_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train Epoch: 1 [0/60160 (0%)]\tLoss: 2.298057\n",
      "Train Epoch: 1 [8192/60160 (14%)]\tLoss: 2.177127\n",
      "Train Epoch: 1 [16384/60160 (27%)]\tLoss: 1.963209\n",
      "Train Epoch: 1 [24576/60160 (41%)]\tLoss: 1.359332\n",
      "Train Epoch: 1 [32768/60160 (54%)]\tLoss: 0.829705\n",
      "Train Epoch: 1 [40960/60160 (68%)]\tLoss: 0.524044\n",
      "Train Epoch: 1 [49152/60160 (82%)]\tLoss: 0.422464\n",
      "Train Epoch: 1 [57344/60160 (95%)]\tLoss: 0.463303\n",
      "\n",
      "Train set: Average loss: 1.2134, Accuracy: 43450/60160 (72%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4049, Accuracy: 8858/10000 (89%)\n",
      "\n",
      "0\n",
      "Train Epoch: 2 [0/60160 (0%)]\tLoss: 0.381216\n",
      "Train Epoch: 2 [8192/60160 (14%)]\tLoss: 0.370956\n",
      "Train Epoch: 2 [16384/60160 (27%)]\tLoss: 0.350118\n",
      "Train Epoch: 2 [24576/60160 (41%)]\tLoss: 0.308901\n",
      "Train Epoch: 2 [32768/60160 (54%)]\tLoss: 0.266912\n",
      "Train Epoch: 2 [40960/60160 (68%)]\tLoss: 0.297197\n",
      "Train Epoch: 2 [49152/60160 (82%)]\tLoss: 0.223786\n",
      "Train Epoch: 2 [57344/60160 (95%)]\tLoss: 0.286311\n",
      "\n",
      "Train set: Average loss: 0.3189, Accuracy: 54422/60160 (90%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3169, Accuracy: 9024/10000 (90%)\n",
      "\n",
      "0\n",
      "Train Epoch: 3 [0/60160 (0%)]\tLoss: 0.417628\n",
      "Train Epoch: 3 [8192/60160 (14%)]\tLoss: 0.267603\n",
      "Train Epoch: 3 [16384/60160 (27%)]\tLoss: 0.293912\n",
      "Train Epoch: 3 [24576/60160 (41%)]\tLoss: 0.256351\n",
      "Train Epoch: 3 [32768/60160 (54%)]\tLoss: 0.198145\n",
      "Train Epoch: 3 [40960/60160 (68%)]\tLoss: 0.202585\n",
      "Train Epoch: 3 [49152/60160 (82%)]\tLoss: 0.213254\n",
      "Train Epoch: 3 [57344/60160 (95%)]\tLoss: 0.238821\n",
      "\n",
      "Train set: Average loss: 0.2286, Accuracy: 55960/60160 (93%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2107, Accuracy: 9372/10000 (94%)\n",
      "\n",
      "0\n",
      "Train Epoch: 4 [0/60160 (0%)]\tLoss: 0.249533\n",
      "Train Epoch: 4 [8192/60160 (14%)]\tLoss: 0.248467\n",
      "Train Epoch: 4 [16384/60160 (27%)]\tLoss: 0.121148\n",
      "Train Epoch: 4 [24576/60160 (41%)]\tLoss: 0.199003\n",
      "Train Epoch: 4 [32768/60160 (54%)]\tLoss: 0.205634\n",
      "Train Epoch: 4 [40960/60160 (68%)]\tLoss: 0.169030\n",
      "Train Epoch: 4 [49152/60160 (82%)]\tLoss: 0.116616\n",
      "Train Epoch: 4 [57344/60160 (95%)]\tLoss: 0.150855\n",
      "\n",
      "Train set: Average loss: 0.1806, Accuracy: 56818/60160 (94%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1465, Accuracy: 9592/10000 (96%)\n",
      "\n",
      "0\n",
      "Train Epoch: 5 [0/60160 (0%)]\tLoss: 0.226488\n",
      "Train Epoch: 5 [8192/60160 (14%)]\tLoss: 0.173007\n",
      "Train Epoch: 5 [16384/60160 (27%)]\tLoss: 0.177663\n",
      "Train Epoch: 5 [24576/60160 (41%)]\tLoss: 0.130753\n",
      "Train Epoch: 5 [32768/60160 (54%)]\tLoss: 0.118984\n",
      "Train Epoch: 5 [40960/60160 (68%)]\tLoss: 0.155384\n",
      "Train Epoch: 5 [49152/60160 (82%)]\tLoss: 0.157710\n",
      "Train Epoch: 5 [57344/60160 (95%)]\tLoss: 0.133641\n",
      "\n",
      "Train set: Average loss: 0.1511, Accuracy: 57328/60160 (95%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1479, Accuracy: 9582/10000 (96%)\n",
      "\n",
      "0\n",
      "Train Epoch: 6 [0/60160 (0%)]\tLoss: 0.126588\n",
      "Train Epoch: 6 [8192/60160 (14%)]\tLoss: 0.111298\n",
      "Train Epoch: 6 [16384/60160 (27%)]\tLoss: 0.101853\n",
      "Train Epoch: 6 [24576/60160 (41%)]\tLoss: 0.138430\n",
      "Train Epoch: 6 [32768/60160 (54%)]\tLoss: 0.132805\n",
      "Train Epoch: 6 [40960/60160 (68%)]\tLoss: 0.106905\n",
      "Train Epoch: 6 [49152/60160 (82%)]\tLoss: 0.168739\n",
      "Train Epoch: 6 [57344/60160 (95%)]\tLoss: 0.153732\n",
      "\n",
      "Train set: Average loss: 0.1297, Accuracy: 57737/60160 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1381, Accuracy: 9576/10000 (96%)\n",
      "\n",
      "0\n",
      "Train Epoch: 7 [0/60160 (0%)]\tLoss: 0.131399\n",
      "Train Epoch: 7 [8192/60160 (14%)]\tLoss: 0.098005\n",
      "Train Epoch: 7 [16384/60160 (27%)]\tLoss: 0.149491\n",
      "Train Epoch: 7 [24576/60160 (41%)]\tLoss: 0.102688\n",
      "Train Epoch: 7 [32768/60160 (54%)]\tLoss: 0.176025\n",
      "Train Epoch: 7 [40960/60160 (68%)]\tLoss: 0.120700\n",
      "Train Epoch: 7 [49152/60160 (82%)]\tLoss: 0.088328\n",
      "Train Epoch: 7 [57344/60160 (95%)]\tLoss: 0.139356\n",
      "\n",
      "Train set: Average loss: 0.1146, Accuracy: 58003/60160 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1097, Accuracy: 9678/10000 (97%)\n",
      "\n",
      "0\n",
      "Train Epoch: 8 [0/60160 (0%)]\tLoss: 0.209864\n",
      "Train Epoch: 8 [8192/60160 (14%)]\tLoss: 0.068374\n",
      "Train Epoch: 8 [16384/60160 (27%)]\tLoss: 0.172457\n",
      "Train Epoch: 8 [24576/60160 (41%)]\tLoss: 0.076889\n",
      "Train Epoch: 8 [32768/60160 (54%)]\tLoss: 0.092858\n",
      "Train Epoch: 8 [40960/60160 (68%)]\tLoss: 0.087112\n",
      "Train Epoch: 8 [49152/60160 (82%)]\tLoss: 0.115997\n",
      "Train Epoch: 8 [57344/60160 (95%)]\tLoss: 0.069294\n",
      "\n",
      "Train set: Average loss: 0.1020, Accuracy: 58252/60160 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "using_DP=False if variants[current_exp] == 0 else True\n",
    "run_experiment(using_DP=using_DP, stddev=variants[current_exp])\n",
    "current_exp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "using_DP=False if variants[current_exp] == 0 else True\n",
    "run_experiment(using_DP=using_DP, stddev=variants[current_exp])\n",
    "current_exp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "using_DP=False if variants[current_exp] == 0 else True\n",
    "run_experiment(using_DP=using_DP, stddev=variants[current_exp])\n",
    "current_exp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "using_DP=False if variants[current_exp] == 0 else True\n",
    "run_experiment(using_DP=using_DP, stddev=variants[current_exp])\n",
    "current_exp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_train_graph(dp_suffix, metric, stddev):\n",
    "    '''\n",
    "    Plots and saves test train graphs for each metric\n",
    "    using_DP: 'dp', 'no_dp'\n",
    "    metric: 'acc', 'loss'\n",
    "    '''\n",
    "\n",
    "    train_metric = history[f'train_{metric}_{dp_suffix}_{str(stddev)}']\n",
    "    test_metric = history[f'test_{metric}_{dp_suffix}_{str(stddev)}']\n",
    "    plt.subplots(facecolor='w')\n",
    "    plt.plot(range(1, len(train_metric) + 1),\n",
    "                train_metric, label=f'Train {metric}')\n",
    "    plt.plot(range(1, len(test_metric) + 1), test_metric, label=f'Test {metric}')\n",
    "    plt.title(f'Test vs Train {metric} for {dp_suffix}: {stddev}')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('No. Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(\n",
    "        f'./results/vis_{metric}_{dp_suffix}_{str(stddev)}.png'\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in variants:\n",
    "    d = 'no_dp' if v == 0 else 'dp'\n",
    "    for m in metrics:\n",
    "        plot_test_train_graph(d, m, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_variants_graph(phase, metric, variants):\n",
    "    '''\n",
    "    Plots and saves test train graphs for each metric\n",
    "    phase: 'test', 'train'\n",
    "    metric: 'acc', 'loss'\n",
    "    '''\n",
    "    \n",
    "    plt.subplots(facecolor='w')\n",
    "    for v in variants:\n",
    "        d = 'no_dp' if v == 0 else 'dp'\n",
    "        dp_metric = history[f'{phase}_{metric}_{d}_{str(v)}']\n",
    "        plt.plot(range(1, len(dp_metric) + 1), dp_metric, label=f'{phase} {metric} {d} stddev{v}')\n",
    "    plt.title(f'DP Comparison {phase} {metric}')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('No. Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(\n",
    "        f'./results/dp_comparison_{phase}_{metric}.png'\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variants_graph('test', 'acc', variants)\n",
    "plot_variants_graph('train', 'loss', variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
