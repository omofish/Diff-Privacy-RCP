{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 - Federated Learning with FederatedDataset\n",
    "\n",
    "Here we introduce a new tool for using federated datasets. We have created a `FederatedDataset` class which is intended to be used like the PyTorch Dataset class, and is given to a federated data loader `FederatedDataLoader` which will iterate on it in a federated fashion.\n",
    "\n",
    "\n",
    "Authors:\n",
    "- Andrew Trask - Twitter: [@iamtrask](https://twitter.com/iamtrask)\n",
    "- Th√©o Ryffel - GitHub: [@LaRiffle](https://github.com/LaRiffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the sandbox that we discovered last lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/opt/conda/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.4.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import syft as sy\n",
    "sy.create_sandbox(globals(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then search for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data = grid.search(\"#boston\", \"#data\")\n",
    "boston_target = grid.search(\"#boston\", \"#target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a model and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = boston_data['alice'][0].shape[1]\n",
    "n_targets = 1\n",
    "\n",
    "model = th.nn.Linear(n_features, n_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we cast the data fetched in a `FederatedDataset`. See the workers which hold part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bob', 'theo', 'jason', 'alice', 'andy', 'jon']\n"
     ]
    }
   ],
   "source": [
    "# Cast the result in BaseDatasets\n",
    "datasets = []\n",
    "for worker in boston_data.keys():\n",
    "    dataset = sy.BaseDataset(boston_data[worker][0], boston_target[worker][0])\n",
    "    datasets.append(dataset)\n",
    "\n",
    "# Build the FederatedDataset object\n",
    "dataset = sy.FederatedDataset(datasets)\n",
    "print(dataset.workers)\n",
    "optimizers = {}\n",
    "for worker in dataset.workers:\n",
    "    optimizers[worker] = th.optim.Adam(params=model.parameters(),lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put it in a `FederatedDataLoader` and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = sy.FederatedDataLoader(dataset, batch_size=32, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we iterate over epochs. You can see how similar this is compared to pure and local PyTorch training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/16 (0%)]\tBatch loss: 7479.215820\n",
      "Train Epoch: 1 [8/16 (50%)]\tBatch loss: 209.227493\n",
      "Train Epoch: 1 [16/16 (100%)]\tBatch loss: 1320.276367\n",
      "Total loss 41841.46326828003\n",
      "Train Epoch: 2 [0/16 (0%)]\tBatch loss: 1603.421387\n",
      "Train Epoch: 2 [8/16 (50%)]\tBatch loss: 230.163727\n",
      "Train Epoch: 2 [16/16 (100%)]\tBatch loss: 897.259888\n",
      "Total loss 16564.1053314209\n",
      "Train Epoch: 3 [0/16 (0%)]\tBatch loss: 1519.917847\n",
      "Train Epoch: 3 [8/16 (50%)]\tBatch loss: 41.038940\n",
      "Train Epoch: 3 [16/16 (100%)]\tBatch loss: 582.256897\n",
      "Total loss 11492.321350097656\n",
      "Train Epoch: 4 [0/16 (0%)]\tBatch loss: 1424.397583\n",
      "Train Epoch: 4 [8/16 (50%)]\tBatch loss: 46.827415\n",
      "Train Epoch: 4 [16/16 (100%)]\tBatch loss: 511.512573\n",
      "Total loss 10174.852855682373\n",
      "Train Epoch: 5 [0/16 (0%)]\tBatch loss: 774.794556\n",
      "Train Epoch: 5 [8/16 (50%)]\tBatch loss: 43.075127\n",
      "Train Epoch: 5 [16/16 (100%)]\tBatch loss: 348.078918\n",
      "Total loss 6883.286212921143\n",
      "Train Epoch: 6 [0/16 (0%)]\tBatch loss: 569.685303\n",
      "Train Epoch: 6 [8/16 (50%)]\tBatch loss: 40.032146\n",
      "Train Epoch: 6 [16/16 (100%)]\tBatch loss: 235.284409\n",
      "Total loss 4329.097854614258\n",
      "Train Epoch: 7 [0/16 (0%)]\tBatch loss: 486.131958\n",
      "Train Epoch: 7 [8/16 (50%)]\tBatch loss: 46.386063\n",
      "Train Epoch: 7 [16/16 (100%)]\tBatch loss: 174.148407\n",
      "Total loss 3041.455394744873\n",
      "Train Epoch: 8 [0/16 (0%)]\tBatch loss: 262.305298\n",
      "Train Epoch: 8 [8/16 (50%)]\tBatch loss: 41.999260\n",
      "Train Epoch: 8 [16/16 (100%)]\tBatch loss: 118.652115\n",
      "Total loss 2045.234426498413\n",
      "Train Epoch: 9 [0/16 (0%)]\tBatch loss: 107.770851\n",
      "Train Epoch: 9 [8/16 (50%)]\tBatch loss: 43.348396\n",
      "Train Epoch: 9 [16/16 (100%)]\tBatch loss: 104.865555\n",
      "Total loss 1651.2761535644531\n",
      "Train Epoch: 10 [0/16 (0%)]\tBatch loss: 49.381298\n",
      "Train Epoch: 10 [8/16 (50%)]\tBatch loss: 49.110809\n",
      "Train Epoch: 10 [16/16 (100%)]\tBatch loss: 81.599937\n",
      "Total loss 1554.8432178497314\n",
      "Train Epoch: 11 [0/16 (0%)]\tBatch loss: 35.399696\n",
      "Train Epoch: 11 [8/16 (50%)]\tBatch loss: 53.009327\n",
      "Train Epoch: 11 [16/16 (100%)]\tBatch loss: 44.455791\n",
      "Total loss 1591.026523590088\n",
      "Train Epoch: 12 [0/16 (0%)]\tBatch loss: 72.693649\n",
      "Train Epoch: 12 [8/16 (50%)]\tBatch loss: 56.597145\n",
      "Train Epoch: 12 [16/16 (100%)]\tBatch loss: 26.901218\n",
      "Total loss 1817.475440979004\n",
      "Train Epoch: 13 [0/16 (0%)]\tBatch loss: 126.101021\n",
      "Train Epoch: 13 [8/16 (50%)]\tBatch loss: 64.709190\n",
      "Train Epoch: 13 [16/16 (100%)]\tBatch loss: 18.469187\n",
      "Total loss 1959.2836627960205\n",
      "Train Epoch: 14 [0/16 (0%)]\tBatch loss: 170.370804\n",
      "Train Epoch: 14 [8/16 (50%)]\tBatch loss: 70.254379\n",
      "Train Epoch: 14 [16/16 (100%)]\tBatch loss: 13.349698\n",
      "Total loss 1946.1879091262817\n",
      "Train Epoch: 15 [0/16 (0%)]\tBatch loss: 200.403931\n",
      "Train Epoch: 15 [8/16 (50%)]\tBatch loss: 70.422707\n",
      "Train Epoch: 15 [16/16 (100%)]\tBatch loss: 13.007267\n",
      "Total loss 1867.2576684951782\n",
      "Train Epoch: 16 [0/16 (0%)]\tBatch loss: 201.179749\n",
      "Train Epoch: 16 [8/16 (50%)]\tBatch loss: 69.432281\n",
      "Train Epoch: 16 [16/16 (100%)]\tBatch loss: 13.619040\n",
      "Total loss 1709.1397800445557\n",
      "Train Epoch: 17 [0/16 (0%)]\tBatch loss: 177.868362\n",
      "Train Epoch: 17 [8/16 (50%)]\tBatch loss: 66.184647\n",
      "Train Epoch: 17 [16/16 (100%)]\tBatch loss: 14.553727\n",
      "Total loss 1498.5373697280884\n",
      "Train Epoch: 18 [0/16 (0%)]\tBatch loss: 146.186646\n",
      "Train Epoch: 18 [8/16 (50%)]\tBatch loss: 60.736488\n",
      "Train Epoch: 18 [16/16 (100%)]\tBatch loss: 15.642874\n",
      "Total loss 1301.9413466453552\n",
      "Train Epoch: 19 [0/16 (0%)]\tBatch loss: 112.405563\n",
      "Train Epoch: 19 [8/16 (50%)]\tBatch loss: 55.759613\n",
      "Train Epoch: 19 [16/16 (100%)]\tBatch loss: 16.566128\n",
      "Total loss 1141.63769865036\n",
      "Train Epoch: 20 [0/16 (0%)]\tBatch loss: 82.186295\n",
      "Train Epoch: 20 [8/16 (50%)]\tBatch loss: 51.664101\n",
      "Train Epoch: 20 [16/16 (100%)]\tBatch loss: 18.086964\n",
      "Total loss 1021.8270893096924\n",
      "Train Epoch: 21 [0/16 (0%)]\tBatch loss: 59.873734\n",
      "Train Epoch: 21 [8/16 (50%)]\tBatch loss: 48.235638\n",
      "Train Epoch: 21 [16/16 (100%)]\tBatch loss: 19.921751\n",
      "Total loss 943.9040460586548\n",
      "Train Epoch: 22 [0/16 (0%)]\tBatch loss: 45.019272\n",
      "Train Epoch: 22 [8/16 (50%)]\tBatch loss: 45.818958\n",
      "Train Epoch: 22 [16/16 (100%)]\tBatch loss: 21.377380\n",
      "Total loss 898.2514629364014\n",
      "Train Epoch: 23 [0/16 (0%)]\tBatch loss: 35.986198\n",
      "Train Epoch: 23 [8/16 (50%)]\tBatch loss: 44.329887\n",
      "Train Epoch: 23 [16/16 (100%)]\tBatch loss: 22.300570\n",
      "Total loss 871.9934024810791\n",
      "Train Epoch: 24 [0/16 (0%)]\tBatch loss: 31.149670\n",
      "Train Epoch: 24 [8/16 (50%)]\tBatch loss: 43.479721\n",
      "Train Epoch: 24 [16/16 (100%)]\tBatch loss: 22.512053\n",
      "Total loss 855.6969985961914\n",
      "Train Epoch: 25 [0/16 (0%)]\tBatch loss: 28.816986\n",
      "Train Epoch: 25 [8/16 (50%)]\tBatch loss: 43.204689\n",
      "Train Epoch: 25 [16/16 (100%)]\tBatch loss: 21.987988\n",
      "Total loss 843.0451135635376\n",
      "Train Epoch: 26 [0/16 (0%)]\tBatch loss: 27.840588\n",
      "Train Epoch: 26 [8/16 (50%)]\tBatch loss: 43.374535\n",
      "Train Epoch: 26 [16/16 (100%)]\tBatch loss: 20.977356\n",
      "Total loss 830.9381046295166\n",
      "Train Epoch: 27 [0/16 (0%)]\tBatch loss: 27.698614\n",
      "Train Epoch: 27 [8/16 (50%)]\tBatch loss: 43.777267\n",
      "Train Epoch: 27 [16/16 (100%)]\tBatch loss: 19.697710\n",
      "Total loss 819.3266677856445\n",
      "Train Epoch: 28 [0/16 (0%)]\tBatch loss: 28.271675\n",
      "Train Epoch: 28 [8/16 (50%)]\tBatch loss: 44.296974\n",
      "Train Epoch: 28 [16/16 (100%)]\tBatch loss: 18.311169\n",
      "Total loss 809.455379486084\n",
      "Train Epoch: 29 [0/16 (0%)]\tBatch loss: 29.619217\n",
      "Train Epoch: 29 [8/16 (50%)]\tBatch loss: 44.808159\n",
      "Train Epoch: 29 [16/16 (100%)]\tBatch loss: 16.949953\n",
      "Total loss 802.436683177948\n",
      "Train Epoch: 30 [0/16 (0%)]\tBatch loss: 31.762707\n",
      "Train Epoch: 30 [8/16 (50%)]\tBatch loss: 45.161144\n",
      "Train Epoch: 30 [16/16 (100%)]\tBatch loss: 15.703325\n",
      "Total loss 798.8001613616943\n",
      "Train Epoch: 31 [0/16 (0%)]\tBatch loss: 34.519653\n",
      "Train Epoch: 31 [8/16 (50%)]\tBatch loss: 45.261898\n",
      "Train Epoch: 31 [16/16 (100%)]\tBatch loss: 14.635211\n",
      "Total loss 798.1661190986633\n",
      "Train Epoch: 32 [0/16 (0%)]\tBatch loss: 37.506603\n",
      "Train Epoch: 32 [8/16 (50%)]\tBatch loss: 45.055531\n",
      "Train Epoch: 32 [16/16 (100%)]\tBatch loss: 13.787066\n",
      "Total loss 799.4299521446228\n",
      "Train Epoch: 33 [0/16 (0%)]\tBatch loss: 40.274479\n",
      "Train Epoch: 33 [8/16 (50%)]\tBatch loss: 44.536480\n",
      "Train Epoch: 33 [16/16 (100%)]\tBatch loss: 13.174376\n",
      "Total loss 801.3405694961548\n",
      "Train Epoch: 34 [0/16 (0%)]\tBatch loss: 42.435032\n",
      "Train Epoch: 34 [8/16 (50%)]\tBatch loss: 43.767586\n",
      "Train Epoch: 34 [16/16 (100%)]\tBatch loss: 12.789760\n",
      "Total loss 802.7965602874756\n",
      "Train Epoch: 35 [0/16 (0%)]\tBatch loss: 43.757652\n",
      "Train Epoch: 35 [8/16 (50%)]\tBatch loss: 42.846413\n",
      "Train Epoch: 35 [16/16 (100%)]\tBatch loss: 12.608981\n",
      "Total loss 803.0210604667664\n",
      "Train Epoch: 36 [0/16 (0%)]\tBatch loss: 44.199081\n",
      "Train Epoch: 36 [8/16 (50%)]\tBatch loss: 41.877331\n",
      "Train Epoch: 36 [16/16 (100%)]\tBatch loss: 12.594717\n",
      "Total loss 801.6591668128967\n",
      "Train Epoch: 37 [0/16 (0%)]\tBatch loss: 43.862022\n",
      "Train Epoch: 37 [8/16 (50%)]\tBatch loss: 40.953648\n",
      "Train Epoch: 37 [16/16 (100%)]\tBatch loss: 12.699488\n",
      "Total loss 798.7003030776978\n",
      "Train Epoch: 38 [0/16 (0%)]\tBatch loss: 42.940086\n",
      "Train Epoch: 38 [8/16 (50%)]\tBatch loss: 40.140816\n",
      "Train Epoch: 38 [16/16 (100%)]\tBatch loss: 12.870275\n",
      "Total loss 794.3799920082092\n",
      "Train Epoch: 39 [0/16 (0%)]\tBatch loss: 41.663544\n",
      "Train Epoch: 39 [8/16 (50%)]\tBatch loss: 39.474632\n",
      "Train Epoch: 39 [16/16 (100%)]\tBatch loss: 13.054811\n",
      "Total loss 789.0800986289978\n",
      "Train Epoch: 40 [0/16 (0%)]\tBatch loss: 40.252934\n",
      "Train Epoch: 40 [8/16 (50%)]\tBatch loss: 38.966850\n",
      "Train Epoch: 40 [16/16 (100%)]\tBatch loss: 13.209705\n",
      "Total loss 783.2138299942017\n",
      "Train Epoch: 41 [0/16 (0%)]\tBatch loss: 38.890709\n",
      "Train Epoch: 41 [8/16 (50%)]\tBatch loss: 38.610085\n",
      "Train Epoch: 41 [16/16 (100%)]\tBatch loss: 13.306667\n",
      "Total loss 777.1491179466248\n",
      "Train Epoch: 42 [0/16 (0%)]\tBatch loss: 37.706745\n",
      "Train Epoch: 42 [8/16 (50%)]\tBatch loss: 38.383514\n",
      "Train Epoch: 42 [16/16 (100%)]\tBatch loss: 13.333900\n",
      "Total loss 771.176609992981\n",
      "Train Epoch: 43 [0/16 (0%)]\tBatch loss: 36.775436\n",
      "Train Epoch: 43 [8/16 (50%)]\tBatch loss: 38.257275\n",
      "Train Epoch: 43 [16/16 (100%)]\tBatch loss: 13.293651\n",
      "Total loss 765.5070610046387\n",
      "Train Epoch: 44 [0/16 (0%)]\tBatch loss: 36.123295\n",
      "Train Epoch: 44 [8/16 (50%)]\tBatch loss: 38.195660\n",
      "Train Epoch: 44 [16/16 (100%)]\tBatch loss: 13.197515\n",
      "Total loss 760.2911081314087\n",
      "Train Epoch: 45 [0/16 (0%)]\tBatch loss: 35.740223\n",
      "Train Epoch: 45 [8/16 (50%)]\tBatch loss: 38.161415\n",
      "Train Epoch: 45 [16/16 (100%)]\tBatch loss: 13.061667\n",
      "Total loss 755.6346373558044\n",
      "Train Epoch: 46 [0/16 (0%)]\tBatch loss: 35.590748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [8/16 (50%)]\tBatch loss: 38.120094\n",
      "Train Epoch: 46 [16/16 (100%)]\tBatch loss: 12.903194\n",
      "Total loss 751.6007103919983\n",
      "Train Epoch: 47 [0/16 (0%)]\tBatch loss: 35.623436\n",
      "Train Epoch: 47 [8/16 (50%)]\tBatch loss: 38.045010\n",
      "Train Epoch: 47 [16/16 (100%)]\tBatch loss: 12.737736\n",
      "Total loss 748.2020888328552\n",
      "Train Epoch: 48 [0/16 (0%)]\tBatch loss: 35.777973\n",
      "Train Epoch: 48 [8/16 (50%)]\tBatch loss: 37.920174\n",
      "Train Epoch: 48 [16/16 (100%)]\tBatch loss: 12.578151\n",
      "Total loss 745.3913125991821\n",
      "Train Epoch: 49 [0/16 (0%)]\tBatch loss: 35.991554\n",
      "Train Epoch: 49 [8/16 (50%)]\tBatch loss: 37.741379\n",
      "Train Epoch: 49 [16/16 (100%)]\tBatch loss: 12.433895\n",
      "Total loss 743.0625686645508\n",
      "Train Epoch: 50 [0/16 (0%)]\tBatch loss: 36.204720\n",
      "Train Epoch: 50 [8/16 (50%)]\tBatch loss: 37.514748\n",
      "Train Epoch: 50 [16/16 (100%)]\tBatch loss: 12.310845\n",
      "Total loss 741.065703868866\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    loss_accum = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        model.send(data.location)\n",
    "        \n",
    "        optimizer = optimizers[data.location.id]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = ((pred.view(-1) - target)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.get()\n",
    "        loss = loss.get()\n",
    "        \n",
    "        loss_accum += float(loss)\n",
    "        \n",
    "        if batch_idx % 8 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBatch loss: {:.6f}'.format(\n",
    "                epoch, batch_idx, len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))            \n",
    "            \n",
    "    print('Total loss', loss_accum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the Repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
